{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcc4922c-6e76-410e-9004-a9c68a9d6259",
   "metadata": {},
   "source": [
    "# Web Mining & Coginitive Computing - MSc 2025 - FS2024\n",
    "## Group Assignment 1\n",
    "\n",
    "Authors: **Bachem**, Kilian;\n",
    "**Mohr**, Otis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5317c2b3-e6ef-464f-be8e-4c8f6820301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all packages needed for the code below\n",
    "\n",
    "import requests # Needed for sending GET requests to the NBB website\n",
    "from bs4 import BeautifulSoup # Needed for finding and extracting elements from the request response\n",
    "import pandas as pd # Needed for generating/analyzing DataFrames\n",
    "import time # Needed for UNIX Timestamp and Sleep Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e7b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dict variable 'links', which holds the NBB website URLs for each notebook brand\n",
    "links = {\n",
    "    \"Acer\": \"https://www.notebooksbilliger.de/notebooks/acer+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"Apple\": \"https://www.notebooksbilliger.de/notebooks/apple+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"Asus\": \"https://www.notebooksbilliger.de/notebooks/asus+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"Dell\": \"https://www.notebooksbilliger.de/notebooks/dell+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"HP\": \"https://www.notebooksbilliger.de/notebooks/hp+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"Huawei\": \"https://www.notebooksbilliger.de/notebooks/huawei+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"Lenovo\": \"https://www.notebooksbilliger.de/notebooks/lenovo+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"MSI\": \"https://www.notebooksbilliger.de/notebooks/msi+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\",\n",
    "    \"Samsung\": \"https://www.notebooksbilliger.de/notebooks/samsung+notebooks/page/1?perPage=50&sort=popularity&order=desc&availability=alle\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3429328-f3c8-4de6-9b36-98293f7dc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom request header that is used by the requests package to retrieve data from the NBB website. \n",
    "# The header suggests to the website that we are retrieving information from a regular browser.\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83aa35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed request for Acer\n",
      "Completed request for Apple\n",
      "Completed request for Asus\n",
      "Completed request for Dell\n",
      "Completed request for HP\n",
      "Completed request for Huawei\n",
      "Completed request for Lenovo\n",
      "Completed request for MSI\n",
      "Completed request for Samsung\n"
     ]
    }
   ],
   "source": [
    "### SCRAPING THE DATA FROM THE WEBSITE ###\n",
    "\n",
    "# Dictionary to store the request response for each brand webpage\n",
    "responses = {}\n",
    "\n",
    "# Next, iterate through all links, try retrieving the content and save them in the responses variable if the server responds with 200.\n",
    "for brand, url in links.items():\n",
    "    # Make a GET request to the URL\n",
    "    req = requests.get(url, headers=headers)\n",
    "\n",
    "    # For the subsequent steps, record the date of retrieval in a variable 'date_retrieved'\n",
    "    date_retrieved = time.strftime('%Y-%m-%d')\n",
    "\n",
    "    # ONLY if the server responds with 'OK' (in other words, the request was successful), go ahead and save the data.\n",
    "    if req.status_code == 200:\n",
    "        # Save the response under a variable name equal to the respective key value\n",
    "        responses[brand] = req\n",
    "        print(f\"Completed request for {brand}\")\n",
    "        time.sleep(6) # Delay of 6 seconds between queries for compliance\n",
    "    else:\n",
    "        # If the request fails, notify us that the request was unsuccessful\n",
    "        print(f\"Error {req.status_code} received when trying to retrieve data for {brand}\")\n",
    "        time.sleep(6) # Delay of 6 seconds between queries for compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374b79a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA EXTRACTION ###\n",
    "\n",
    "# Create a new list in which the products for all brands will be stored later. \n",
    "all_data = []\n",
    "    \n",
    "# Iterate through the server response for each brand page and filter out the relevant data of the first 10 products.\n",
    "for brand, data in responses.items():\n",
    "\n",
    "    # Create a BeautifulSoup object from the request html code\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "\n",
    "    # Assuming that every product is inside a HTML list element with the class 'product-listing__row'.\n",
    "    # Find 10 elements max which match the criteria, and save them in a new variable, 'products'.\n",
    "    products = soup.find_all('li', class_='product-listing__row', limit=10)\n",
    "\n",
    "    # Create a (temporary) list that will collect the product entries for one brand\n",
    "    brand_data = []\n",
    "\n",
    "    # Iterate through the products identified in the current soup object and extract only the desired data\n",
    "    for product in products:\n",
    "\n",
    "        # A few days inside the assignment, we noticed that NBB sometimes markets notebooks with\n",
    "        # an idential name under different price points (e.g., Black Week Deals or 'B-Ware').\n",
    "        # We therefore need a distinguishing label so we can compare the data later:\n",
    "        product_id_element = product.find('div', class_='product-card')\n",
    "        product_id = product_id_element[\"data-product-id\"] if product_id_element else \"Product ID not found\"\n",
    "\n",
    "        # The timestamp is already known and will be added to the dict directly.\n",
    "        # The vendor is also already known, no need to extract it again.\n",
    "\n",
    "        # First, find the product title in the div with the class name 'product-card__product-heading-title\n",
    "        title_element = product.find('div', class_='product-card__product-heading-title')\n",
    "        # Strip only the text and save it in the 'title' variable. If the title_element is empty, return a \"Not Found\" failsafe\n",
    "        title = title_element.text.strip() if title_element else \"Title Not Found\"\n",
    "\n",
    "        # Exact same method for the product price, also with a failsafe.\n",
    "        price_element = product.find('div', class_='js-product-price')\n",
    "        price = float(price_element.text.strip().replace('â‚¬','').replace('.','').replace(',','.')) if price_element else \"Price Not Found\"\n",
    "\n",
    "        # Extracting Availability is more difficult, since the text can live in multiple elements for some products.\n",
    "        # We first find the overarching wrapper for all elements which can be part of the availability string\n",
    "        availability_wrapper = product.find('div', class_='product-card__availability')\n",
    "        \n",
    "        # Then initialize a temporary list to hold the texts\n",
    "        availability_texts = []\n",
    "        \n",
    "        # Then check if the element was found\n",
    "        if availability_wrapper:\n",
    "            # Find all elements nested inside the 'product-card__availability' div which could hold text, iterate through them and...\n",
    "            for element in availability_wrapper.find_all():\n",
    "                # Append the text of each nested element to the list\n",
    "                availability_texts.append(element.get_text(strip=True))\n",
    "                # Lastly, combine the texts from the nested elements\n",
    "                availability = ''.join(availability_texts)\n",
    "        else:\n",
    "            availability = \"Availability not found\"\n",
    "\n",
    "        # Then, extract the evaluation (star rating). This is a bit more tricky, since we are only interested in the sum of filled stars.\n",
    "        # Thus, we count the number of star images in the rating which are ending in 'full.svg', therefore leaving out empty stars.\n",
    "        rating_images = product.findAll('img', class_='rating__star')\n",
    "        evaluation = sum(1 for img in rating_images if 'full.svg' in img['src'])\n",
    "\n",
    "        # Lastly, extract the rating count. Same method as in the beginning, with a failsafe.\n",
    "        # Also, getting rid of the parentheses and converting String into int\n",
    "        ratingcount_element = product.find('span', class_='rating__count')\n",
    "        ratingcount = int(ratingcount_element.text.strip().replace('(','').replace(')','')) if ratingcount_element else \"Rating Not Found\"\n",
    "\n",
    "        # For each item, create a (temporary) dictionary which aggregates the extracted information...\n",
    "        item_data = {\n",
    "            'product-id': product_id,\n",
    "            'date-retrieved': date_retrieved, # Directly filled in from global variable (date of the GET request)\n",
    "            'vendor': brand, # Directly filled in from loop parameter\n",
    "            'title': title,\n",
    "            'price-eur': price,\n",
    "            'availability': availability,\n",
    "            'evaluation': evaluation,   \n",
    "            'ratingcount': ratingcount\n",
    "        }\n",
    "\n",
    "        #... and then add it to the list of items for the brand in the current iteration.\n",
    "        brand_data.append(item_data)\n",
    "\n",
    "    # After all items have been extracted for one brand, add the list of items to the all_data variable, in form of a dict.\n",
    "    all_data.extend(brand_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0333e36e-5240-4dd3-8b9a-81339d478e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 2-assignment-1-2024-03-22.csv\n"
     ]
    }
   ],
   "source": [
    "### SAVING DATAFRAME AS CSV ###\n",
    "\n",
    "# Creating a pandas DataFrame from the all_data list\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Saving the DataFrame in a .csv file.\n",
    "# The Date Timestamp will be included in the file name, so that we will not accidentally overwrite existing dbs.\n",
    "csv_file_name = f'2-assignment-1-{date_retrieved}.csv'\n",
    "df.to_csv(csv_file_name, index=False)\n",
    "print(f\"Data saved to {csv_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f427214f-e160-4ed5-9856-5b4bddf1fb62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vendor</th>\n",
       "      <th>title</th>\n",
       "      <th>price-eur_old</th>\n",
       "      <th>price-eur_new</th>\n",
       "      <th>price-change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Dell</td>\n",
       "      <td>Dell Latitude 3540 (RNHKD)</td>\n",
       "      <td>888.99</td>\n",
       "      <td>896.99</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Dell</td>\n",
       "      <td>Dell Latitude 5440 (840T3)</td>\n",
       "      <td>1195.99</td>\n",
       "      <td>1198.99</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>MSI</td>\n",
       "      <td>MSI Thin GF63 12VF-290</td>\n",
       "      <td>888.00</td>\n",
       "      <td>1299.00</td>\n",
       "      <td>411.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   vendor                       title  price-eur_old  price-eur_new  \\\n",
       "24   Dell  Dell Latitude 3540 (RNHKD)         888.99         896.99   \n",
       "25   Dell  Dell Latitude 5440 (840T3)        1195.99        1198.99   \n",
       "58    MSI      MSI Thin GF63 12VF-290         888.00        1299.00   \n",
       "\n",
       "    price-change  \n",
       "24           8.0  \n",
       "25           3.0  \n",
       "58         411.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### PRICE COMPARISON BETWEEN TWO DATASETS ###\n",
    "\n",
    "# Since a lot could go wrong when reading the files and merging the tables, we will catch all errors\n",
    "# and print an error message for debugging purposes and robustness.\n",
    "try:\n",
    "    df1 = pd.read_csv('2-assignment-1-2024-03-21.csv')\n",
    "    df2 = pd.read_csv('2-assignment-1-2024-03-22.csv')\n",
    "\n",
    "    # Check if we have the necessary columns for the shortlist and throw an error if not\n",
    "    required_columns = ['product-id', 'vendor', 'title', 'price-eur']\n",
    "    if not all(col in df1.columns and col in df2.columns for col in required_columns):\n",
    "        print(\"One or both files are missing required columns.\")\n",
    "    else:\n",
    "\n",
    "        # If everything is clear, proceed to merge the datasets based on the unique product IDs. Keep both the old and the new price in the table.\n",
    "        merged_df = pd.merge(df1, df2, on=['product-id', 'vendor', 'title'], suffixes=('_old', '_new'))\n",
    "        \n",
    "        # Create a shortlist with only the items for which the old and the new price deviate\n",
    "        price_changes = merged_df[merged_df['price-eur_old'] != merged_df['price-eur_new']].copy()\n",
    "\n",
    "        # Also create a new column, \"price-change\", that shows the price difference\n",
    "        price_changes['price-change'] = price_changes['price-eur_new'] - price_changes['price-eur_old']\n",
    "\n",
    "        # For the shortlist, only show the most relevant columns (kick out ids and ratings from the view)\n",
    "        price_changes_shortlist = price_changes[['vendor', 'title', 'price-eur_old', 'price-eur_new', 'price-change']]\n",
    "\n",
    "        # If no price change has occurred, show a message. Otherwise, display the shortlist as a table.\n",
    "        if price_changes_shortlist.empty:\n",
    "            print(\"No price changes detected.\")\n",
    "        else:\n",
    "            display(price_changes_shortlist)\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while comparing the data: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
